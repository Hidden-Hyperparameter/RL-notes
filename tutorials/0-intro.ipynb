{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Introduction to RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**什么是RL?**\n",
    "\n",
    "> Reinforcement learning (RL) is an **interdisciplinary area** of **machine learning** and **optimal control** concerned with how an intelligent agent **ought to take actions** in a **dynamic environment** in order to **maximize the cumulative reward**. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning. (Wikipedia)\n",
    "\n",
    "简单来说，RL就是一个研究 **如何在和环境交互中最大化自己的利益，或者“reward”** 的学科。RL中重要的元素是**agent**和**environment**，二者形成一个闭环：\n",
    "- agent的输入是environment的反馈，输出它的应对，一般称为**action**；\n",
    "- environment接收agent的反应，并进行一个演化。演化结果给出新的 **状态(state)** 。此外，为了给予agent一个目标，环境还要给agent一个**reward**。\n",
    "\n",
    "换句话说，agent在每一个state下面给出action，而环境根据这个action就会到达一个新的state。而agent的整体目标，如上所述，是 **最大化reward的时间和**。\n",
    "\n",
    "![](./assets/0-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Life is a MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP\n",
    "\n",
    "在介绍RL的基本概念之前，我们首先要介绍MDP的概念。MDP是**Markov Decision Process**的缩写，是强化学习的基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个MDP包括：\n",
    "- 一个状态空间(state space) $S$\n",
    "- 一个动作空间(action space) $A$\n",
    "- 一个奖励函数(reward function) $r$\n",
    "    - 具体地，对于一个state $s\\in S$和一个action $a\\in A$，这一函数给出一个实数 $r(s,a)$，代表着在state $s$下面采取action $a$获得的“奖励”。\n",
    "- 一个状态转移函数(transition function) $p$\n",
    "    - 具体地，对于一个state $s\\in S$和一个action $a\\in A$，这一函数给出一个概率分布$p(s'|s,a)$，代表着在state $s$下面采取action $a$之后，下一个state是$s'$的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "这里的“Markov”体现在哪里呢？我们可以设想一个agent在和环境逐渐地交互：\n",
    "- agent初始时处于一个state $s_0$,这是环境给定的；\n",
    "- agent根据$s_0$采取一个action $a_0$；\n",
    "- 环境根据$s_0$和$a_0$给出一个reward $r_0$，并且给出下一个state $s_1$。此时，agent就处于state $s_1$；\n",
    "- agent再根据$s_1$采取一个action $a_1$。不断由此往复……\n",
    "\n",
    "可以看到，在这一过程中，只要第$t$轮的$s_t,a_t$给定了，环境就可以决定$s_{t+1}$，而不需要$s_0,a_0,s_1,\\cdots,s_{t-1},a_{t-1}$这些信息！这就是Markov性质的体现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么，“Decision”又是指什么呢？可以看到，上面的描述只包含了环境的部分；而我们的agent的介入方式也很简单，就是对于一个state $s$，给出一个**政策** (policy)，也就是action的分布。我们可以把它记作$\\pi(a|s)$。\n",
    "\n",
    "> Q: 为什么政策只需要当前的state就可以做出决定呢？和历史一定无关吗？\n",
    ">\n",
    "> A: 这是也是因为我们Markov的假设。你说的这种情况叫做“Non-markovian agent”。虽然这更普遍，但很多经验表明，non-markovian的agent并不比我们这样的agent做的好很多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POMDP\n",
    "\n",
    "POMDP是Partially Observed MDP的简称。这对应一个虽然略复杂，但更普适的问题陈述：环境并不一定向我们完全袒露心怀，而是只给我们一部分信息。这样的问题在实际中更为常见。比如说，在物理的控制问题中，我们获得一张快照时，就丧失了作为物理系统的state所必须的速度信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于POMDP，我们的policy就不能再是state的函数了。我们后面介绍POMDP的问题的时候也会提到，有两种类型的policy:\n",
    "\n",
    "- memoryless: $\\pi(a_t|o_t)$\n",
    "- history-dependent: $\\pi(a_t|o_1,o_2,\\cdots,o_t)$\n",
    "\n",
    "对于前者，我们有一个形象的图来展示：\n",
    "\n",
    "![](../lecture/notes/assets/2-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的箭头代表着依赖关系，比如$o_t$依赖于且仅依赖于$s_t$。这样的问题就更加复杂了，因为observation本身不是markovian的——$o_t$不仅依赖于$o_{t-1},a_{t-1}$，而且依赖于$o_{t-2},a_{t-2},\\cdots,o_1,a_1$。\n",
    "\n",
    "在接下来的讨论中，如果不加说明，我们都考虑fully observed的MDP。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP Simulators\n",
    "\n",
    "MDP是一个如此广泛的概念，以至于大部分生活中的问题，在合适地定义state后，都可以变成一个在MDP中最大化cumulative reward的问题。正因如此，**RL基本上就是研究求解MDP的学科**。\n",
    "\n",
    "但是，在研究RL时，如果真的要和环境进行交互，可能会有很大的代价（想一想为什么？）为此，`gym`这一包创造了许多模拟环境，供人们“刷分”，也就是把RL算法投入真正应用前先看一看大体上这一方法好不好。我们接下来来看一下gym的使用。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
