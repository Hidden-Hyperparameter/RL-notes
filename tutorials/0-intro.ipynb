{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Introduction to RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本讲内容十分有趣，我们以一个完全没有学习过RL的初学者的视角来看RL问题，并自然地引入许多常见的RL算法。尽管这里并不会详细介绍细节（细节会在后面的讲内部介绍），但是会建立起对这些算法的直观理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**什么是RL?**\n",
    "\n",
    "> Reinforcement learning (RL) is an **interdisciplinary area** of **machine learning** and **optimal control** concerned with how an intelligent agent **ought to take actions** in a **dynamic environment** in order to **maximize the cumulative reward**. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning. (Wikipedia)\n",
    "\n",
    "简单来说，RL就是一个研究 **如何在和环境交互中最大化自己的利益，或者“reward”** 的学科。RL中重要的元素是**agent**和**environment**，二者形成一个闭环：\n",
    "- agent的输入是environment的反馈，输出它的应对，一般称为**action**；\n",
    "- environment接收agent的反应，并进行一个演化。演化结果给出新的 **状态(state)** 。此外，为了给予agent一个目标，环境还要给agent一个**reward**。\n",
    "\n",
    "换句话说，agent在每一个state下面给出action，而环境根据这个action就会到达一个新的state。而agent的整体目标，如上所述，是 **最大化reward的时间和**。\n",
    "\n",
    "![](./assets/0-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Life is a MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP\n",
    "\n",
    "在介绍RL的基本概念之前，我们首先要介绍MDP的概念。MDP是**Markov Decision Process**的缩写，是强化学习的基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个MDP包括：\n",
    "- 一个状态空间(state space) $S$\n",
    "- 一个动作空间(action space) $A$\n",
    "- 一个奖励函数(reward function) $r$\n",
    "    - 具体地，对于一个state $s\\in S$和一个action $a\\in A$，这一函数给出一个实数 $r(s,a)$，代表着在state $s$下面采取action $a$获得的“奖励”。\n",
    "- 一个状态转移函数(transition function) $p$\n",
    "    - 具体地，对于一个state $s\\in S$和一个action $a\\in A$，这一函数给出一个概率分布$p(s'|s,a)$，代表着在state $s$下面采取action $a$之后，下一个state是$s'$的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "这里的“Markov”体现在哪里呢？我们可以设想一个agent在和环境逐渐地交互：\n",
    "- agent初始时处于一个state $s_0$,这是环境给定的；\n",
    "- agent根据$s_0$采取一个action $a_0$；\n",
    "- 环境根据$s_0$和$a_0$给出一个reward $r_0$，并且给出下一个state $s_1$。此时，agent就处于state $s_1$；\n",
    "- agent再根据$s_1$采取一个action $a_1$。不断由此往复……\n",
    "\n",
    "可以看到，在这一过程中，只要第$t$轮的$s_t,a_t$给定了，环境就可以决定$s_{t+1}$，而不需要$s_0,a_0,s_1,\\cdots,s_{t-1},a_{t-1}$这些信息！这就是Markov性质的体现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么，“Decision”又是指什么呢？可以看到，上面的描述只包含了环境的部分；而我们的agent的介入方式也很简单，就是对于一个state $s$，给出一个**政策** (policy)，也就是action的分布。我们可以把它记作$\\pi(a|s)$。\n",
    "\n",
    "> Q: 为什么政策只需要当前的state就可以做出决定呢？和历史一定无关吗？\n",
    ">\n",
    "> A: 这是也是因为我们Markov的假设。你说的这种情况叫做“Non-markovian agent”。虽然这更普遍，但很多经验表明，non-markovian的agent并不比我们这样的agent做的好很多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POMDP\n",
    "\n",
    "POMDP是Partially Observed MDP的简称。这对应一个虽然略复杂，但更普适的问题陈述：环境并不一定向我们完全袒露心怀，而是只给我们一部分信息。这样的问题在实际中更为常见。比如说，在物理的控制问题中，我们获得一张快照时，就丧失了作为物理系统的state所必须的速度信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于POMDP，我们的policy就不能再是state的函数了。我们后面介绍POMDP的问题的时候也会提到，有两种类型的policy:\n",
    "\n",
    "- memoryless: $\\pi(a_t|o_t)$\n",
    "- history-dependent: $\\pi(a_t|o_1,o_2,\\cdots,o_t)$\n",
    "\n",
    "对于前者，我们有一个形象的图来展示：\n",
    "\n",
    "![](../lecture/notes/assets/2-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的箭头代表着依赖关系，比如$o_t$依赖于且仅依赖于$s_t$。这样的问题就更加复杂了，因为observation本身不是markovian的——$o_t$不仅依赖于$o_{t-1},a_{t-1}$，而且依赖于$o_{t-2},a_{t-2},\\cdots,o_1,a_1$。\n",
    "\n",
    "在接下来的讨论中，如果不加说明，我们都考虑fully observed的MDP。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP Simulators\n",
    "\n",
    "MDP是一个如此广泛的概念，以至于大部分生活中的问题，在合适地定义state后，都可以变成一个在MDP中最大化cumulative reward的问题。正因如此，**RL基本上就是研究求解MDP的学科**。\n",
    "\n",
    "但是，在研究RL时，如果真的要和环境进行交互，可能会有很大的代价（想一想为什么？）为此，OpenAI 的 `gym` 包创造了许多模拟环境，供人们“刷分”，也就是把RL算法投入真正应用前先看一看大体上这一方法好不好。我们接下来来看一下gym的使用。\n",
    "\n",
    "> gym就是一坨答辨。 ——某学长\n",
    "\n",
    "gym以它的不稳定和不兼容著称，而其渲染部分则是更符合这一描述。因此，在下面的代码开始前，让我们：\n",
    "- 安装[requirements.txt](./requirements.txt)内的包；\n",
    "- 运行`bash install.sh`；\n",
    "- 如果在下面的代码的运行中，你遇到和渲染相关的错误，请不要贸然相信GPT或copilot给你的命令，因为它们有可能会让你的图形界面崩溃，或者显卡驱动失效。建议在各种论坛网站上找一些解决方案。\n",
    "- 如果你奋斗了很久还是看不到图片，建议你不要再尝试了，而是把所有含有`render`相关的代码注释掉，并观察后面的输出。在之后一般情况下，我们都不会用到渲染。\n",
    "- 最后，祈祷！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Good luck!'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\"Gym, I cry out to You; make haste to me! Give output to my code when I cry out to You. Let my library be set before You as binaries, the lifting up of my hands as drivers for graphics.\" and 'Good luck!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial observation: [ 0.0273956  -0.00611216  0.03585979  0.0197368 ]\n",
      "Initial info: {}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAFeCAYAAAAYIxzjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAKXUlEQVR4nO3dTatdVx3H8f/e55zcPtyWxmJpFW0nlUB8GhUFQVBwKJ3kFRTpS3HqtC+h0LkTB0oVNKAVRGliqza0VkltmocmN/ecs5eD60BB7tkm3ruT/ft8xjs3/0nC96619l5da60VABCrn3oAAGBaYgAAwokBAAgnBgAgnBgAgHBiAADCiQEACCcGACCcGACAcGIAAMKJAQAIJwYAIJwYAIBwYgAAwokBAAgnBgAgnBgAgHBiAADCiQEACCcGACCcGACAcGIAAMKJAQAIJwYAIJwYAIBwYgAAwokBAAgnBgAgnBgAgHBiAADCiQEACCcGACCcGACAcGIAAMKJAQAIJwYAIJwYAIBwYgAAwokBAAgnBgAgnBgAgHBiAADCiQEACCcGACCcGACAcGIAAMKJAQAIJwYAIJwYAIBwYgAAwokBAAgnBgAgnBgAgHBiAADCiQEACCcGACCcGACAcGIAAMKJAQAIJwYAIJwYAIBwYgAAwokBAAgnBgAgnBgAgHBiAADCiQEACCcGACCcGACAcGIAAMKJAQAIJwYAIJwYAIBwYgAAwokBAAgnBgAgnBgAgHBiAADCiQEACCcGACCcGACAcGIAAMKJAQAIJwYAIJwYAIBwYgAAwokBAAgnBgAgnBgAgHBiAADCiQEACCcGACCcGACAcGIAAMKJAQAIJwYAIJwYAIBwYgAAwokBAAgnBgAgnBgAgHBiAADCiQEACCcGACCcGACAcGIAAMKJAQAIJwYAIJwYAIBwYgAAwokBAAgnBgAgnBgAgHBiAADCiQEACCcGACCcGACAcGIAAMKJAQAIJwYAIJwYAIBwYgAAwokBAAgnBgAgnBgAgHBiAADCiQEACCcGACCcGACAcGIAAMKJAQAIJwYAIJwYAIBwYgAAwokBAAgnBgAgnBgAgHBiAADCiQEACCcGACCcGACAcGIAAMKJAQAIJwYAIJwYAIBwYgAAwokBAAgnBgAgnBgAgHBiAADCLaceAHgw3PzbO/XhWz8+9plHnnq2vvjNC6c0EXBaxABQrbU6vPVxXb/yu2Of2xx8ekoTAafJNgFQ1VoN283UUwATEQNAVbVqw3bqIYCJiAGgWmvVtuupxwAmIgaAqqpqWysDkEoMAEdnBgZnBiCVGACqyjYBJBMDwL/ODFgZgFRiAKgq2wSQTAwAVa18ZwCCiQGgWtkmgGRiAKhqPjoEycQAUMN2XXeu/fX4h7q+Hv3M505nIOBUiQGghvXdun31vWOf6fq+nnjuS6c0EXCaxAAwUlfdwkWnMEdiABitX6ymHgE4AWIAGKWrqn4pBmCOxAAwTtdVb5sAZkkMAKN1tglglsQAMFInBmCmxAAwWt/bJoA5EgPAOJ0DhDBXYgAYyXcGYK7EADCa7wzAPIkBYJSuxADMlRiAcK21cQ92tglgrsQAUMN2M/UIwITEAFBtu556BGBCYgCoQQxANDEAiAEIJwaAGjZiAJKJAcCZAQgnBgArAxBODADODEA4MQDUsDmcegRgQmIAcGYAwokBiNd8gRDCiQHAygCEEwOQrrW6+vYvdj729IvfOIVhgCmIAaA2B7d2PrN6/OwpTAJMQQwAo/TL1dQjACdEDACj9AsxAHMlBoBRrAzAfIkBYJR+cWbqEYATIgaAUTorAzBbYgAYpV8spx4BOCFiABilX9omgLkSA8Ao3iaA+RIDwCjeJoD5EgPAKN4mgPkSA8AonW0CmC0xAOFaa6Oe8zYBzJcYgHBtuxn3YFfVdd3JDgNMQgxAuGG7nnoEYGJiAMIdxcC4rQJgnsQAhBs2VgYgnRiAcM02AcQTAxDOygAgBiCcA4SAGIBwtgkAMQDhhs2hlwkgnBiAcMPYjw4BsyUGIJwzA4AYgHDD5nDqEYCJiQEId/fG1dp1aGD1+NnqOv9dwFz51w3hPvnzWzufOfvC16vr3VoIcyUGgJ26xbLKhYUwW2IA2KlfLEsNwHyJAWCnbrGaegTgBIkBYKd+sSorAzBfYgDYqV8sq9MCMFtiANipW9omgDkTA8BOfW+bAOZMDAA7dcuVFoAZEwPATkevFgJzJQaAnbxNAPMmBoCdfIoY5k0MQLDW2o4rio50tglg1sQABGvbTe26sbDqaIOg86EBmC0xAMGGYVPVxqwNAHMmBiDY0coAkE4MQLC23VSzMgDxxAAEG7brGnNmAJg3MQDBhmHrzAAgBiCZMwNAlRiAaG1wZgAQAxCtbde2CQAxAMmGkR8dAuZNDEAwrxYCVWIAog0OEAIlBiDazQ8v17A+OPaZxz77fC0f3T+liYApuIoMHlKttdput/f1M+7e/Ee14fifsXrsqWrdsjabe19FWCwWLjqCB1jXbBjCQ+ny5ct1/vz5+/oZP/zBd+rbX3v+2Gd+8us/1Y/euFgf37xzT3/H3t5e3bhxo/reQiQ8qKwMwEOqtXZfv61XVbVh9+8Ch+ttHa7X9/x3LRaLe/pzwOkRA0Bt2rL+fveFujM8UV212l9cq2fOvFddV7XZbmuwgAizJgYgXGtd/ebG9+rm5uk6bHvVVasz/Z26uv5CfXn/57XeDKNWEICHlxiAYEP19cvr369PNs9U1dEBv1ZVd4f9ev/gXPU11Hr7eysDMHNO9ECw39787n+EwL9r1dd7B+fr3VvnxADMnBiAeMe98tfVejvUYJsAZk0MAMfabAZ3GcHMiQHgWN4mgPkTAxDsq/s/rf3FtfrvNxe2+vzepXp2+QfbBDBzYgCCLbt1feupN+rJxUe17O5W1VBdDbXq7tRzZ96tr+z/rIbh0MoAzJxXCyHYr95+vz759KA27Z364ODF+nR7troa6snlR3XrkT/WX6rqnQ+uTT0mcMJG303w6quvnvQswP/g+vXr9frrr089xk5939crr7zioiKYyGuvvbbzmdExcPHixfseCPj/uXLlSl24cGHqMXZarVb15ptvigGYyEsvvbTzGbcWwkPq0qVLde7cuanH2Glvb69u377t1kJ4gPnXCQDhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIRzayE8pPb39+vll1+eeoydVqvV1CMAO7ibAADC2SYAgHBiAADCiQEACCcGACCcGACAcGIAAMKJAQAIJwYAIJwYAIBwYgAAwokBAAgnBgAgnBgAgHBiAADCiQEACCcGACCcGACAcGIAAMKJAQAIJwYAIJwYAIBwYgAAwokBAAgnBgAgnBgAgHBiAADCiQEACCcGACCcGACAcGIAAMKJAQAIJwYAIJwYAIBwYgAAwokBAAgnBgAgnBgAgHBiAADCiQEACCcGACCcGACAcGIAAMKJAQAIJwYAIJwYAIBwYgAAwokBAAgnBgAgnBgAgHBiAADCiQEACCcGACCcGACAcGIAAMKJAQAIJwYAIJwYAIBwYgAAwokBAAgnBgAgnBgAgHBiAADCiQEACCcGACCcGACAcGIAAMKJAQAIJwYAIJwYAIBwYgAAwokBAAgnBgAgnBgAgHBiAADCiQEACCcGACCcGACAcGIAAMKJAQAI909rws+EOmphMgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from utils import show_image\n",
    "# Create the CartPole environment with 'rgb_array' mode\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "\n",
    "# Reset the environment\n",
    "state,info = env.reset(seed=42)\n",
    "print('Initial state:',state)\n",
    "print('Initial info:',info)\n",
    "\n",
    "# Render the environment and display the image\n",
    "image = env.render()\n",
    "show_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你成功看到了上面的输出和图片，那么恭喜你，你看到了环境`env`在初始化时的state $s_0$。根据gym的[官方文档](https://gymnasium.farama.org/environments/classic_control/cart_pole/)，这一环境的名字叫做`CartPole`，描述的是一个匀质杆一段铰接在一个置于光滑导轨的小车上。我们看到的`Initial state`中这四个参数的含义如下：\n",
    "- Cart Position\n",
    "- Cart Velocity\n",
    "- Pole Angle\n",
    "- Pole Angular Velocity\n",
    "\n",
    "（注：`gym`包中管这个state叫做`observation`，比如`env.observation_space`实际上是state space的意思）。上面的结果是决定性的，因为我们手动设置了`seed`为42。接下来，让我们再看看action是什么："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('action space',\n",
       " [np.int64(0),\n",
       "  np.int64(1),\n",
       "  np.int64(1),\n",
       "  np.int64(0),\n",
       "  np.int64(0),\n",
       "  np.int64(1),\n",
       "  np.int64(0),\n",
       "  np.int64(1),\n",
       "  np.int64(0),\n",
       "  np.int64(0)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.seed(42)\n",
    "env.observation_space.seed(42)\n",
    "'action space',[env.action_space.sample() for _ in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据官方文档，0代表向左推一下车，而1代表向右推一下。那么，假设对于初始的状态，我们连续地向右推，会变成什么样？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time stamp 0 reward: 1.0\n",
      "time stamp 1 reward: 1.0\n",
      "time stamp 2 reward: 1.0\n",
      "time stamp 3 reward: 1.0\n",
      "time stamp 4 reward: 1.0\n",
      "time stamp 5 reward: 1.0\n",
      "time stamp 6 reward: 1.0\n",
      "time stamp 7 reward: 1.0\n",
      "time stamp 8 reward: 1.0\n",
      "time stamp 9 reward: 1.0\n",
      "Stopped at timestamp 9 due to done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video controls  >\n",
       " <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
       " Your browser does not support the video tag.\n",
       " </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import show_video\n",
    "import numpy as np\n",
    "frames = []\n",
    "for t in range(100):\n",
    "    image = env.render()\n",
    "    frames.append(image)\n",
    "\n",
    "    action = np.int64(1) # always take the \"push right\" action.\n",
    "    next_state, reward, done, truncated, info = env.step(action) # 'step' is the function that gives the transition.\n",
    "    state = next_state\n",
    "    print('time stamp',t,'reward:',reward)\n",
    "    if done or truncated:\n",
    "        print('Stopped at timestamp',t,'due to','done' if done else 'truncated')\n",
    "        break\n",
    "\n",
    "show_video(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "很直观——如果我们一直推这一个车，杆就会向左倒下。此时，因为杆和竖直方向的夹角达到了某个临界值，`env`自动判断为游戏失败而结束。这就是`done`为`True`的原因。\n",
    "\n",
    "同时，我们也可以看到`reward`的数值——对于每一个存活下来的一步，reward都是1。也就是说，我们的目的就是活得越长越好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，让我们建立一个简单的agent，对这一任务作出一个难度上的基础感受。我们的`TrivialAgent`的思路很简单——如果杆偏左并且角速度向左，那么我们就往左推车；如果杆偏右并且角速度向右，那么我们就往右推车。最后，对于其他情况，因为这取决于未知的物理参数，我们不妨随机来选取action。这十分粗暴，但不妨让我们就看看它能做得怎样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrivialAgent:\n",
    "\n",
    "    def __init__(self,action_space):\n",
    "        self.action_space = action_space\n",
    "    \n",
    "    def get_action(self,state):\n",
    "        _,_,theta,omg = state\n",
    "        if theta < 0 and omg < 0:\n",
    "            return 0\n",
    "        if theta > 0 and omg > 0:\n",
    "            return 1\n",
    "        return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminate at time 499 due to truncated\n",
      "Terminate at time 499 due to truncated\n",
      "Terminate at time 347 due to done\n",
      "Terminate at time 412 due to done\n",
      "Terminate at time 395 due to done\n",
      "Terminate at time 482 due to done\n",
      "Terminate at time 499 due to truncated\n",
      "Terminate at time 249 due to done\n",
      "Terminate at time 499 due to truncated\n",
      "Terminate at time 310 due to done\n",
      "Average cumulative reward 420.1\n",
      "results: [500.0, 500.0, 348.0, 413.0, 396.0, 483.0, 500.0, 250.0, 500.0, 311.0]\n"
     ]
    }
   ],
   "source": [
    "agent = TrivialAgent(env.action_space)\n",
    "NUM_ROLLOUTS = 10\n",
    "results = []\n",
    "for rollout in range(NUM_ROLLOUTS):\n",
    "    cum_reward = 0\n",
    "    state,_ = env.reset()\n",
    "    for t in range(1000):\n",
    "        action = agent.get_action(state) # let our agent to decide what to do\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        state = next_state\n",
    "        cum_reward += reward\n",
    "        if done or truncated:\n",
    "            print('Terminate at time',t,'due to', 'done' if done else 'truncated')\n",
    "            results.append(cum_reward)\n",
    "            break\n",
    "print('Average cumulative reward',sum(results)/len(results))\n",
    "print('results:',results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用`TrivialAgent`，我们解锁了一个新“技能”——`truncated`！这个情况是因为环境具有一个有限的“**horizon**”，也就是到达这个时间后就会自动结束，类似于“游戏胜利”。这样，总的cumulative reward才有上限。我们看到，就这样一个愚蠢的agent就已经可以做到84%的最优解了！因此，这一任务确实是简单的。\n",
    "\n",
    "我们还可以看到，不同组数据之间reward（即存活时间）可以相差很远——最大值是最小值的2倍！的确，你开始认识到RL了——一个接着一个的state迭代使得整个体系具有**巨大的方差**。我们后面会更加具体地讨论这一点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，让我们欣赏一下我们的第一个“agent”的舞蹈，来结束这一小节吧！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped at timestamp 499 due to truncated\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStopped at timestamp\u001b[39m\u001b[38;5;124m'\u001b[39m,t,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdue to\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m done \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncated\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mshow_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Github/RL-notes/tutorials/utils.py:24\u001b[0m, in \u001b[0;36mshow_video\u001b[0;34m(frames)\u001b[0m\n\u001b[1;32m     22\u001b[0m ani \u001b[38;5;241m=\u001b[39m animation\u001b[38;5;241m.\u001b[39mFuncAnimation(fig, update, frames\u001b[38;5;241m=\u001b[39mframes, interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, blit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/tmp/video.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mani\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mffmpeg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m plt\u001b[38;5;241m.\u001b[39mclose(fig)\n\u001b[1;32m     26\u001b[0m display(Video(video_path, embed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m~/anaconda3/envs/DYY/lib/python3.10/site-packages/matplotlib/animation.py:1089\u001b[0m, in \u001b[0;36mAnimation.save\u001b[0;34m(self, filename, writer, fps, dpi, codec, bitrate, extra_args, metadata, extra_anim, savefig_kwargs, progress_callback)\u001b[0m\n\u001b[1;32m   1087\u001b[0m         progress_callback(frame_number, total_frames)\n\u001b[1;32m   1088\u001b[0m         frame_number \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1089\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrab_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msavefig_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DYY/lib/python3.10/site-packages/matplotlib/animation.py:351\u001b[0m, in \u001b[0;36mMovieWriter.grab_frame\u001b[0;34m(self, **savefig_kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfig\u001b[38;5;241m.\u001b[39mset_size_inches(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_w, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h)\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# Save the figure data to the sink, using the frame format and dpi.\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_proc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mdpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msavefig_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DYY/lib/python3.10/site-packages/matplotlib/figure.py:3395\u001b[0m, in \u001b[0;36mFigure.savefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   3393\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes:\n\u001b[1;32m   3394\u001b[0m         _recursively_make_axes_transparent(stack, ax)\n\u001b[0;32m-> 3395\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DYY/lib/python3.10/site-packages/matplotlib/backend_bases.py:2204\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2201\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[1;32m   2202\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[1;32m   2203\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[0;32m-> 2204\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mprint_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2205\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2206\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfacecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2207\u001b[0m \u001b[43m            \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2208\u001b[0m \u001b[43m            \u001b[49m\u001b[43morientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morientation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2209\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbbox_inches_restore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_bbox_inches_restore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2210\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2211\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[0;32m~/anaconda3/envs/DYY/lib/python3.10/site-packages/matplotlib/backend_bases.py:2054\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m     optional_kws \u001b[38;5;241m=\u001b[39m {  \u001b[38;5;66;03m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[1;32m   2051\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medgecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morientation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2052\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox_inches_restore\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m   2053\u001b[0m     skip \u001b[38;5;241m=\u001b[39m optional_kws \u001b[38;5;241m-\u001b[39m {\u001b[38;5;241m*\u001b[39minspect\u001b[38;5;241m.\u001b[39msignature(meth)\u001b[38;5;241m.\u001b[39mparameters}\n\u001b[0;32m-> 2054\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mwraps(meth)(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2056\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[1;32m   2057\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m meth\n",
      "File \u001b[0;32m~/anaconda3/envs/DYY/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:432\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_raw\u001b[0;34m(self, filename_or_obj, metadata)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata not supported for raw/rgba\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 432\u001b[0m \u001b[43mFigureCanvasAgg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m renderer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_renderer()\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39mopen_file_cm(filename_or_obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fh:\n",
      "File \u001b[0;32m~/anaconda3/envs/DYY/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:387\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\u001b[38;5;241m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\n\u001b[1;32m    386\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m nullcontext()):\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;66;03m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;66;03m# don't forget to call the superclass.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdraw()\n",
      "File \u001b[0;32m~/anaconda3/envs/DYY/lib/python3.10/site-packages/matplotlib/artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 95\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[1;32m     97\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[0;32m~/anaconda3/envs/DYY/lib/python3.10/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/DYY/lib/python3.10/site-packages/matplotlib/figure.py:3162\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3159\u001b[0m             \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[1;32m   3161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[0;32m-> 3162\u001b[0m     \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3165\u001b[0m     renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigure\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   3166\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/DYY/lib/python3.10/site-packages/matplotlib/image.py:132\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 132\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/DYY/lib/python3.10/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/DYY/lib/python3.10/site-packages/matplotlib/axes/_base.py:3137\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m artists_rasterized:\n\u001b[1;32m   3135\u001b[0m     _draw_rasterized(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, artists_rasterized, renderer)\n\u001b[0;32m-> 3137\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3140\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   3141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/DYY/lib/python3.10/site-packages/matplotlib/image.py:132\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 132\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/DYY/lib/python3.10/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/DYY/lib/python3.10/site-packages/matplotlib/image.py:653\u001b[0m, in \u001b[0;36m_ImageBase.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    651\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mdraw_image(gc, l, b, im, trans)\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 653\u001b[0m     im, l, b, trans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_magnification\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m im \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    656\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mdraw_image(gc, l, b, im)\n",
      "File \u001b[0;32m~/anaconda3/envs/DYY/lib/python3.10/site-packages/matplotlib/image.py:952\u001b[0m, in \u001b[0;36mAxesImage.make_image\u001b[0;34m(self, renderer, magnification, unsampled)\u001b[0m\n\u001b[1;32m    949\u001b[0m transformed_bbox \u001b[38;5;241m=\u001b[39m TransformedBbox(bbox, trans)\n\u001b[1;32m    950\u001b[0m clip \u001b[38;5;241m=\u001b[39m ((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_clip_box() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mbbox) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_clip_on()\n\u001b[1;32m    951\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mbbox)\n\u001b[0;32m--> 952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_A\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_bbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmagnification\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munsampled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munsampled\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DYY/lib/python3.10/site-packages/matplotlib/image.py:567\u001b[0m, in \u001b[0;36m_ImageBase._make_image\u001b[0;34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    565\u001b[0m         output_alpha \u001b[38;5;241m=\u001b[39m _resample(  \u001b[38;5;66;03m# resample alpha channel\u001b[39;00m\n\u001b[1;32m    566\u001b[0m             \u001b[38;5;28mself\u001b[39m, A[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m3\u001b[39m], out_shape, t, alpha\u001b[38;5;241m=\u001b[39malpha)\n\u001b[0;32m--> 567\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43m_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# resample rgb channels\u001b[39;49;00m\n\u001b[1;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_rgb_to_rgba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m     output[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m=\u001b[39m output_alpha  \u001b[38;5;66;03m# recombine rgb and alpha\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;66;03m# output is now either a 2D array of normed (int or float) data\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;66;03m# or an RGBA array of re-sampled input\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/DYY/lib/python3.10/site-packages/matplotlib/image.py:208\u001b[0m, in \u001b[0;36m_resample\u001b[0;34m(image_obj, data, out_shape, transform, resample, alpha)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     resample \u001b[38;5;241m=\u001b[39m image_obj\u001b[38;5;241m.\u001b[39mget_resample()\n\u001b[0;32m--> 208\u001b[0m \u001b[43m_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m                \u001b[49m\u001b[43m_interpd_\u001b[49m\u001b[43m[\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m                \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m                \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m                \u001b[49m\u001b[43mimage_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_filternorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m                \u001b[49m\u001b[43mimage_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_filterrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frames = []\n",
    "[env.reset() for _ in range(2)] # a tiny trick here :)\n",
    "observation,_ = env.reset()\n",
    "for t in range(1000):\n",
    "    image = env.render()\n",
    "    frames.append(image)\n",
    "\n",
    "    action = agent.get_action(observation)\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    observation = next_state\n",
    "    if done or truncated:\n",
    "        print('Stopped at timestamp',t,'due to','done' if done else 'truncated')\n",
    "        break\n",
    "\n",
    "show_video(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 MDP Solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有一种非常启发性的讨论RL问题的方法：我们从简单到难，逐渐引出RL的各种方法。现在，就让我们来这样做。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Known MDP\n",
    "\n",
    "注意前面的问题中（从`gym`的演示中也可以看到），环境的transition function和reward function都是完全未知的。但考虑到我们对MDP问题还没有太多的了解，我们不妨放松一步，观察一下如果这些函数都是**已知的**，我们**作为一个人**该如何找到最好的策略呢？\n",
    "\n",
    "这可能说起来略微抽象，但我们可以考虑如下的具体问题：在一个$4\\times 4$的方格表里有一个特殊的格子，到达那里时游戏结束并且获得奖励5，而其他地方的奖励都是-1（为了督促你尽快到达终点而非随机乱走）。transition也很简单：在每一个格子(state)处，可以选择向一个方向走(action)，然后就走到了下一个state。（如果在终点那么不管如何走都到达end state）。\n",
    "\n",
    "注意根据我们的假设，这个MDP是已知的，也就是我们知道那个特殊的格子在哪里。此时，如果不是agent而是人来解决这一问题，你该如何解决？很明显，你可以给出一个决定性的最优策略：假设我们在那个格子旁边的时候，我们就走过去；进一步，如果在再旁边一点的8个格子处，我们就走到前一次的4个格子……这样，递归地，如果确定了距离格子的Manhattan distance (定义为 $\\text{MHT}((x_1,y_1),(x_2,y_2))=|x_1-x_2|+|y_1-y_2|$)为$n$的所有点的策略，那么距离格子为$n+1$的所有点的策略也就确定了。因此，我们可以给出一个完整的策略。给定一个具体的 $4\\times 4$ 表格和那个特殊的格子，我们就可以给出这个最优策略。一个例子如下所示。\n",
    "\n",
    "|$\\rightarrow$ | $\\rightarrow$ | $\\downarrow$ | $\\leftarrow$|\n",
    "|--|--|--|--|\n",
    "| $\\rightarrow$| $\\to$ | $\\star$ | $\\leftarrow$ |\n",
    "|$\\rightarrow$ | $\\rightarrow$ | $\\uparrow$ | $\\leftarrow$ |\n",
    "| $\\rightarrow$| $\\uparrow$ | $\\uparrow$ | $\\leftarrow$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解决了这一问题，我们接下来试着把问题稍微一般化。假设transition并非像上面一样是确定的，而是**随机的**：在一个格子处向上走，只有70%的概率会真正走到上边——剩下的30%里，会随机走到周围剩余的格子里。注意这才是常态，因为我们在定义MDP的时候给出的就是transition probability $p(s'|s,a)$ 而非一个决定性的函数。我们接下来就来试着求解这一问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value Iteration\n",
    "\n",
    "那么，这样我们应该怎么办呢？容易想到，我们可以和上面完全类似地使用一种**递推关系**的方法解决这一问题。我们首先假设我们处于**最优的策略**；接下来，我们给每一个格子定义一个**价值**(value)，代表着如果我们是从这个格子出发，那么最好能达到多少分。很明显，我们有 $V(s_{\\star})=5$，其中$s_\\star$代表那个特殊的格子。而对于其他格子，我们则要试着把它的价值和周围几个格子的价值联系起来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||B| | |\n",
    "|--|--|--|--|\n",
    "|C| A | $\\star$ | |\n",
    "| | D| |  |\n",
    "| | | | |\n",
    "\n",
    "让我们以格子$A$为例来研究。我们虽然不知道最优策略是什么，但是我们总可以假设他的一般形式：\n",
    "\n",
    "$$\n",
    "\\pi^\\star(a|s_A)=\\begin{cases}\\rightarrow&,\\text{w.p. }p_1 \\\\\\leftarrow&,\\text{w.p. }p_2 \\\\ \\uparrow&,\\text{w.p. }p_3 \\\\ \\downarrow&,\\text{w.p. }p_4\\end{cases}\n",
    "$$\n",
    "\n",
    "其中$\\pi^\\star$的$\\star$代表这是最优策略。那么，我们就可以根据MDP和$V$的定义写出$V(A)$的表达式：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "V(s_A)&=p_1\\left[-1+(70\\%\\times V(s_{\\star})+10\\%\\times V(s_B)+10\\%\\times V(s_C)+10\\%\\times V(s_D))\\right]\\\\\n",
    "&\\quad +p_2\\left[-1+(70\\%\\times V(s_C)+10\\%\\times V(s_\\star)+10\\%\\times V(s_B)+10\\%\\times V(s_D))\\right]\\\\\n",
    "&\\quad +p_3\\left[-1+(70\\%\\times V(s_B)+10\\%\\times V(s_\\star)+10\\%\\times V(s_C)+10\\%\\times V(s_D))\\right]\\\\\n",
    "&\\quad +p_4\\left[-1+(70\\%\\times V(s_D)+10\\%\\times V(s_B)+10\\%\\times V(s_\\star)+10\\%\\times V(s_C))\\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "但是我们又注意到$\\pi^\\star$是最优策略，因为这是我们的假设！因此，我们就可以“确定”下来$p_1,p_2,p_3,p_4$：一定是中括号里最大的那个action具有100%的概率，而其他都是0%概率。因此，我们就给出了：\n",
    "\n",
    "$$\n",
    "V(s_A)=-1+\\max\\Big\\{70\\%\\times V(s_{\\star})+10\\%\\times V(s_B)+10\\%\\times V(s_C)+10\\%\\times V(s_D),70\\%\\times V(s_C)+10\\%\\times V(s_\\star)+10\\%\\times V(s_B)+10\\%\\times V(s_D), \\\\ 70\\%\\times V(s_B)+10\\%\\times V(s_\\star)+10\\%\\times V(s_C)+10\\%\\times V(s_D), 70\\%\\times V(s_D)+10\\%\\times V(s_B)+10\\%\\times V(s_\\star)+10\\%\\times V(s_C)\\Big\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似地，对于其他的每一个格子，我们都可以列出这样的一个方程。16个方程16个未知数，理论上一定可以求解；但是一系列max的存在使得这个问题变得有些复杂。一个投机取巧的方式是说，假设我们并不是直接求解，而是通过“赋值”的方式来近似求解呢？这就是**Value Iteration**算法的基本思想——看，你已经接触到了第一个RL算法！\n",
    "\n",
    "让我们总结一下，并把上面的讨论一般化。我们给出**Value Iteration**：\n",
    "\n",
    "> **Value Iteration Algorithm**\n",
    "\n",
    "1. Init values $V(s)$ randomly for each state.\n",
    "2. Repeat until convergence:\n",
    "    - For each state $s$:\n",
    "        - Update $V(s)$ by\n",
    "        $$\n",
    "        V(s)\\leftarrow r(s,a)+\\max_{a\\in A}\\mathbb{E}_{s'\\sim p(s'|s,a)}[V(s')]\n",
    "        $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，让我们来试一试吧！（为了使得代码简洁一些，我们把一些细节的内容放到了[utils_0_intro.py](./utils_0_intro.py)里面）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|0.00|0.00|0.00|0.00|\n",
       "|-|-|-|-|\n",
       "|0.00|0.00|0.00|0.00|\n",
       "|0.00|0.00|0.00|0.00|\n",
       "|0.00|0.00|0.00|0.00|"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils_0_intro import Table\n",
    "from utils import add_method_to_class\n",
    "\n",
    "@add_method_to_class(Table)\n",
    "def iteration(self:Table):\n",
    "    \"\"\"\n",
    "    Perform the Value Iteration algorithm above for one step, and update `self.V`.\n",
    "    \"\"\"\n",
    "    new_V = [[0]*self.size for _ in range(self.size)]\n",
    "    for i in range(self.size):\n",
    "        for j in range(self.size):\n",
    "            if (i,j) == self.special:\n",
    "                new_V[i][j] = 5\n",
    "                continue\n",
    "            l = []\n",
    "            for action in self.get_possible_action((i,j)):\n",
    "                weights = self.get_transition((i,j),action)\n",
    "                l.append(sum([w*self.V[x][y] for (x,y),w in weights.items()]))\n",
    "            new_V[i][j] = -1 + max(l) # The formula above\n",
    "    self.V = new_V\n",
    "\n",
    "t = Table()\n",
    "t.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "|-1.00|-1.00|-1.00|-1.00|\n",
       "|-|-|-|-|\n",
       "|-1.00|-1.00|5.00|-1.00|\n",
       "|-1.00|-1.00|-1.00|-1.00|\n",
       "|-1.00|-1.00|-1.00|-1.00|"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "|0.02|1.07|2.95|1.66|\n",
       "|-|-|-|-|\n",
       "|0.30|2.76|5.00|2.95|\n",
       "|-0.67|0.61|2.76|1.07|\n",
       "|-2.74|-0.67|0.30|0.02|"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "|0.36|1.62|3.03|2.04|\n",
       "|-|-|-|-|\n",
       "|1.04|2.86|5.00|3.03|\n",
       "|-0.28|1.26|2.86|1.62|\n",
       "|-1.22|-0.28|1.04|0.36|"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "|0.47|1.64|3.05|2.05|\n",
       "|-|-|-|-|\n",
       "|1.08|2.90|5.00|3.05|\n",
       "|-0.10|1.29|2.90|1.64|\n",
       "|-1.13|-0.10|1.08|0.47|"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "|0.48|1.65|3.05|2.06|\n",
       "|-|-|-|-|\n",
       "|1.09|2.90|5.00|3.05|\n",
       "|-0.09|1.31|2.90|1.65|\n",
       "|-1.09|-0.09|1.09|0.48|"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for time in range(21):\n",
    "    t.iteration()\n",
    "    if time % 5 == 0:\n",
    "        print('Iteration',time)\n",
    "        t.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现，这个算法很快就收敛了！的确——在后面的讲中，我们会论证这一算法的收敛性。但接下来，有了这些value，我们应该如何计算我们需要的最优policy呢（还记得我们一开始假设自己处于最优policy，但求解的目标实际上是找到这个policy）？这其实也很简单，因为我们已经得到了结论，我们的action一定是最大化reward $r(s,a)$ 和next value $V(s')$ 的期望值的和。\n",
    "\n",
    "这一过程也被称为**Policy Extraction**：\n",
    "\n",
    "$$\n",
    "\\pi^\\star(a|s)=\\delta\\left(a=\\arg\\max_{a\\in A}\\left(r(s,a)+\\mathbb{E}_{s'\\sim p(s'|s,a)}[V(s')]\\right)\\right)\n",
    "$$\n",
    "\n",
    "其中$\\delta$代表单点分布。\n",
    "\n",
    "让我们实现这一点："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|$\\rightarrow$|$\\rightarrow$|$\\downarrow$|$\\downarrow$|\n",
       "|-|-|-|-|\n",
       "|$\\rightarrow$|$\\rightarrow$|$\\star$|$\\leftarrow$|\n",
       "|$\\rightarrow$|$\\uparrow$|$\\uparrow$|$\\uparrow$|\n",
       "|$\\uparrow$|$\\uparrow$|$\\uparrow$|$\\uparrow$|"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@add_method_to_class(Table)\n",
    "def extract_action(self:Table):\n",
    "    self.action = [[None]*self.size for _ in range(self.size)]\n",
    "    for i in range(self.size):\n",
    "        for j in range(self.size):\n",
    "            if (i,j) == self.special:\n",
    "                self.action[i][j] = '\\\\star'\n",
    "                continue\n",
    "            best_act = None\n",
    "            best_val = -float('inf')\n",
    "            for action in self.get_possible_action((i,j)):\n",
    "                weights = self.get_transition((i,j),action)\n",
    "                new_val = -1 + sum([w*self.V[x][y] for (x,y),w in weights.items()])\n",
    "                if new_val > best_val:\n",
    "                    best_val = new_val\n",
    "                    best_act = action\n",
    "            self.action[i][j] = self.get_display_str(best_act)\n",
    "\n",
    "t.extract_action()\n",
    "t.display(display_type='action')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到，这确实像是一个最优的策略。至此，我们就完成了Value Iteration方法的讨论，并第一次完整地解决了一个已知MDP下的问题！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What about a larger MDP?\n",
    "\n",
    "上面的方法很直观，并且也很实用。但是，好景不长——在上面的问题里，state space的大小是16。但如果你的梦想是像Alpha Go一样处理围棋，你常见的state space可能是超过$10^{100}$以上！对于这样庞大的state space，就算在宇宙中的每一粒原子上都写下一个value，也不够储存。更为甚者，如果我们处理像之前CartPole一样的问题，我们的state是连续的，就更是无从下手了。\n",
    "\n",
    "但当我们想到这里的时候，我们一下子就发现了——我们可以使用DL中的技术，**用一个神经网络近似$V(s)$** ！这个网络输入$s$，输出对应的value。考虑到神经网络的“压缩”作用（想想ChatGPT这一模型的大小肯定比它的训练数据加起来小很多倍！），我们就可以处理大规模的MDP问题了。上面的算法中不需要修改任何地方——只需要在更新的时候把赋值换为梯度下降就可以了。（当然，如果具体实现起来可能有很多细节，比如为了避免直接训练这样`batch=1`的不稳定，我们需要采集很多数据再做一波训练，等等）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但敏锐的你可能会发现其中的漏洞：神经网络的拟合本身会带来一个误差。此时，如果依然采用迭代的方法，我们真的还能保证收敛吗？残酷的现实是：**不**。这样的方法（也称为**fitted Value iteration**）并没有一个收敛的保证。实际上，它们不仅理论上不收敛，在实验上也展现着很大的方差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以再实操一个例子：“玩像素”。我们有一个$10\\times 10$的方格表，其中有22个棋子是我们可以操控的。我们希望这些棋子排成一个指定的汉字：\"兵\"。\n",
    "\n",
    "一开始，我们的所有棋子都在左上角；而对于每一个state，我们可以采取一个action $(x,y,d)$，其中$(x,y)$代表需要移动的棋子的位置（如果没有棋子那么无效）；而$d$和之前一样是0,1,2,3中的一个，代表上下左右移动（如果无法移动那么也无效）。为了简化问题，我们认为transition是确定的。最后，reward $r(s,a)$ 取为只和$s$有关。假设$s$和最终target image的共同像素数（不计算重数）是$N$，那么\n",
    "\n",
    "$$\n",
    "r(s,a)=\\begin{cases}\\frac{N-22}{220}&,N<22\\\\100&,N=22\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target image:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| | | | | | | | | | |\n",
       "|-|-|-|-|-|-|-|-|-|-|\n",
       "| | | | | |█| | | | |\n",
       "| | | | |█| | | | | |\n",
       "| | | |█| | | | | | |\n",
       "| | | |█|█|█|█|█| | |\n",
       "| | | |█| |█| | | | |\n",
       "| |█|█|█|█|█|█|█|█| |\n",
       "| | | |█| |█| | | | |\n",
       "| | |█| | | |█| | | |\n",
       "| | | | | | | | | | |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our initial state:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "|█| | | | | | | | | |\n",
       "|-|-|-|-|-|-|-|-|-|-|\n",
       "| | | | | | | | | | |\n",
       "| | | | | | | | | | |\n",
       "| | | | | | | | | | |\n",
       "| | | | | | | | | | |\n",
       "| | | | | | | | | | |\n",
       "| | | | | | | | | | |\n",
       "| | | | | | | | | | |\n",
       "| | | | | | | | | | |\n",
       "| | | | | | | | | | |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial reward -0.1\n"
     ]
    }
   ],
   "source": [
    "from utils_0_intro import PixelGame\n",
    "from utils import add_method_to_class\n",
    "pgame = PixelGame()\n",
    "print('Target image:')\n",
    "pgame.display_target()\n",
    "print('Our initial state:')\n",
    "pgame.display()\n",
    "print('Initial reward',pgame.get_reward())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们就来设计一个简单的网络$V(s)$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,out_channels=32,kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32,out_channels=8,kernel_size=3)\n",
    "        self.mlp = nn.Linear(288,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        compute the estimated Value given state `x`.\n",
    "        \"\"\"\n",
    "        x = nn.LeakyReLU(0.1)(self.conv1(x))\n",
    "        x = nn.LeakyReLU(0.1)(self.conv2(x))\n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们来准备训练数据。如何准备呢？我们发现，应该尽可能地遍历很多的state，而且遍历到的state要**具有代表性**。比如最简单地，如果没有任何一个state有着非零的reward，我们的网络就不可能学到任何东西。最后，回顾一下我们的训练过程是\n",
    "\n",
    "$$\n",
    "V(s)\\leftarrow r(s,a)+\\max_{a\\in A}\\mathbb{E}_{s'\\sim p(s'|s,a)}[V(s')]\n",
    "$$\n",
    "\n",
    "因此，我们应该对于一个state作所有可能的action，并采集大量的$s'$。也就是说，我们的“数据集”是大量$(s,a,r(s,a),s')$组成的对。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathered states: 460\n"
     ]
    }
   ],
   "source": [
    "# To make sure that states are representative, we sample states based on scores:\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "@add_method_to_class(PixelGame)\n",
    "def sample_given_score(self:PixelGame,match_num,sample_num=20):\n",
    "    \"\"\"\n",
    "    Sample some states given `match_num`.\n",
    "\n",
    "    Args:\n",
    "        `match_num`: the number of states that match the target image.\n",
    "        `sample_num`: the number of states to sample.\n",
    "    \"\"\"\n",
    "    other_pixels = [(i,j) for i in range(10) for j in range(10) if (i,j) not in self.target_pixel_pos]\n",
    "    states = []\n",
    "    for _ in range(sample_num):\n",
    "        common = random.sample(self.target_pixel_pos,match_num)\n",
    "        others = random.sample(other_pixels,22-match_num)\n",
    "        states.append(common + others)\n",
    "    return states\n",
    "\n",
    "states = []\n",
    "for match_num in range(23):\n",
    "    states.extend(pgame.sample_given_score(match_num=match_num))\n",
    "print('Gathered states:',len(states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total transition datas 40480\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "transition_datas = []\n",
    "for s in states:\n",
    "    r = pgame.get_reward(s)\n",
    "    for a in pgame.get_actions(s):\n",
    "        transition_datas.append((s,a,r,pgame.get_transition(s,a)))\n",
    "\n",
    "print('Total transition datas',len(transition_datas))\n",
    "\n",
    "# to demonstrate our data distribute evenly on rewards, we plot a histogram:\n",
    "plt.hist([pgame.get_reward(x) for x,_,_,_ in transition_datas],bins=50,range=(-0.1,0),rwidth=0.8) and None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batchify the data\n",
    "batch_size = 128\n",
    "class MyDataset(Dataset):\n",
    "    __init__ = lambda self,datas: setattr(self,'datas',datas)\n",
    "    __len__ = lambda self: len(self.datas)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.datas[idx]\n",
    "    \n",
    "def collate(transitions):\n",
    "    states = []\n",
    "    rewards = []\n",
    "    next_states = []\n",
    "    terminated = []\n",
    "    for t in transitions:\n",
    "        states.append(pgame.as_tensor(t[0]))\n",
    "        rewards.append(t[2])\n",
    "        if t[3] is not None:\n",
    "            next_states.append(pgame.as_tensor(t[3]))\n",
    "            terminated.append(0.)\n",
    "        else:\n",
    "            next_states.append(torch.zeros(1,10,10))\n",
    "            terminated.append(1.)\n",
    "    return {\n",
    "        'states': torch.stack(states,dim=0),\n",
    "        'rewards': torch.tensor(rewards,dtype=torch.float).unsqueeze(-1),\n",
    "        'next_states': torch.stack(next_states,dim=0),\n",
    "        'terminated': torch.tensor(terminated,dtype=torch.float).unsqueeze(-1)\n",
    "    }\n",
    "\n",
    "dataset = MyDataset(transition_datas)\n",
    "dataloader = DataLoader(dataset,collate_fn=collate,batch_size=128,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好的，我们终于可以开始训练了！回顾一下，最外围的循环是Value iteration，就像之前那样；而和之前不同的是，我们在内部又添加了一个神经网络拟合的循环。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss 80.6320: 100%|██████████| 317/317 [00:20<00:00, 15.53it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of the specific state 98.88929748535156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1: loss 0.0634: 100%|██████████| 317/317 [00:19<00:00, 15.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of the specific state 99.0673599243164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 2: loss 0.0616: 100%|██████████| 317/317 [00:22<00:00, 14.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of the specific state 98.8299560546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 3: loss 0.0642: 100%|██████████| 317/317 [00:20<00:00, 15.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of the specific state 98.91757202148438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 4: loss 0.0621:  20%|█▉        | 63/317 [00:03<00:15, 16.14it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(dataloader) \u001b[38;5;28;01mas\u001b[39;00m bar:\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m bar:\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     12\u001b[0m             target \u001b[38;5;241m=\u001b[39m it[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrewards\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m net(it[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnext_states\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m it[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterminated\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;66;03m# [batch,1]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/DYY/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/DYY/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/DYY/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/DYY/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 15\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(transitions)\u001b[0m\n\u001b[1;32m     13\u001b[0m terminated \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m transitions:\n\u001b[0;32m---> 15\u001b[0m     states\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     16\u001b[0m     rewards\u001b[38;5;241m.\u001b[39mappend(t[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Github/RL-notes/tutorials/utils_0_intro.py:115\u001b[0m, in \u001b[0;36mPixelGame.as_tensor\u001b[0;34m(self, position)\u001b[0m\n\u001b[1;32m    113\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos:\n\u001b[0;32m--> 115\u001b[0m     out[pos[\u001b[38;5;241m0\u001b[39m],pos[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available(): device = 'cuda'\n",
    "if torch.backends.mps.is_available(): device = 'mps'\n",
    "from tqdm import tqdm\n",
    "random_state = transition_datas[1145][0] # here we select a state to monitor the convergence of value iteration.\n",
    "values_for_that_state = []\n",
    "\n",
    "net = Net().to(device)\n",
    "opt = Adam(net.parameters(),lr=1e-3)\n",
    "for iteration in range(10):\n",
    "    losses = []\n",
    "    with tqdm(dataloader) as bar:\n",
    "        for it in bar:\n",
    "            with torch.no_grad():\n",
    "                target = it['rewards'].to(device) + net(it['next_states'].to(device)) * (1 - it['terminated'].to(device)) # [batch,1]\n",
    "            val = net(it['states'].to(device)) # [batch,1]\n",
    "            loss = F.mse_loss(val,target)\n",
    "            losses.append(loss.detach().cpu().item())\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            # print(net.mlp.weight[:10][:10])\n",
    "            # print(net.mlp.weight.grad[:10][:10])\n",
    "            # assert False\n",
    "            opt.step()\n",
    "            bar.set_description(f'Iteration {iteration}: loss {sum(losses[-10:])/len(losses[-10:]):.4f}')\n",
    "    value = net(pgame.as_tensor(random_state).unsqueeze(0)).item()\n",
    "    print('Value of the specific state',value)\n",
    "    values_for_that_state.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
